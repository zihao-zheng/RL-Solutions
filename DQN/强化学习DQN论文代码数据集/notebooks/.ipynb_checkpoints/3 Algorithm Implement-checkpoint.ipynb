{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. 算法分析\n",
    "\n",
    "2. agent结构\n",
    "\n",
    "3. loss计算\n",
    "\n",
    "4. 训练过程\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 算法分析\n",
    "\n",
    "首先，我们对比2013年和2015年两篇paper的算法的不同之处\n",
    "\n",
    "<img src=\"./imgs/Old DQN.png\"  width=\"550\" height=\"500\" align=\"left\" />\n",
    "<img src=\"./imgs/Better DQN.png\"  width=\"400\" height=\"500\" align=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个很重要的区别是2015年的版本中引入了target action_value function $\\hat{Q}$ 以及其独立的参数$\\theta^-$，而且$\\theta^-$的更新频率比$\\theta$低很多。这个方法能帮助训练中的target更加稳定，从而提升训练的效果，我们也将直接实现2015年的算法\n",
    "\n",
    "## 2) agent结构\n",
    "\n",
    "### 1) 功能分析\n",
    "\n",
    "首先我们应该考虑我们需要一个怎样的agent。\n",
    "\n",
    "在算法中，可以看到agent需要做的事情有：\n",
    "  * 选择action，随机或非随机\n",
    "  * 更新$Q$的参数$\\theta$\n",
    "  * 更新$\\hat{Q}$的参数$\\theta^-$\n",
    "  \n",
    "而其他的一些部分与agent相关，但并不一定要融入agent内部：\n",
    "  * 数据预处理\n",
    "  * Replay Buffer\n",
    "  \n",
    "在这次的代码中，我们选择将**Replay Buffer**部分放入agent内部，这并不是必须的，但是可以让代码稍微简单一些。做出这样的选择后，我们可以大致确定agent类的主要部分：\n",
    "\n",
    "  * 初始化，`agent.__init__(self, *args, **kwargs)`\n",
    "    * 设置各种参数，如学习率，$\\gamma$\n",
    "    * 初始化Q-Network $Q, \\hat{Q}$\n",
    "    * 初始化Replay Buffer\n",
    "  * 选择行动，`agent.act(self, state, eps)`\n",
    "    * 随机选择\n",
    "    * 非随机选择\n",
    "  * 更新网络参数，`agent.learn(self)`\n",
    "    * 从Replay Buffer中抽取mini-batch\n",
    "    * 计算loss\n",
    "    * 更新参数$\\theta$\n",
    "    * 如果需要，更新参数$\\theta^-$\n",
    "    \n",
    "下面我们一个一个部分分析\n",
    "\n",
    "### 2) 初始化\n",
    "\n",
    "这个部分比较简单，只需要合理的传参就可以完成参数的初始化和网络的初始化，需要考虑的部分有：\n",
    "  * 如何让两个网络权重一致\n",
    "    * 方法1：使用`state_dict`方法，由$Q$进行`save`，然后`load`到$\\hat{Q}$\n",
    "    * 方法2：对每个layer的`data`进行提取，然后进行计算，最后使用`copy_`方法导入$\\hat{Q}$\n",
    "  * 如何设置Replay Buffer\n",
    "    * 方法1：直接使用`collections`的`deque`类，只做储存功能\n",
    "    * 方法2：在方法1的基础上，拓展成一个类，支持写入和抽样方法\n",
    "    \n",
    "**调整网络权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0925,  0.2964,  0.1767, -0.3285, -0.1470],\n",
       "                      [ 0.3120, -0.4361,  0.1911,  0.3987,  0.4299],\n",
       "                      [ 0.3245,  0.3840, -0.2999,  0.3200,  0.0381]])),\n",
       "             ('fc1.bias', tensor([ 0.2561, -0.3150,  0.3420])),\n",
       "             ('fc2.weight', tensor([[-0.5426, -0.5191,  0.4505],\n",
       "                      [-0.2780, -0.4256,  0.3189],\n",
       "                      [ 0.3753,  0.4871,  0.4037]])),\n",
       "             ('fc2.bias', tensor([ 0.2724, -0.1105, -0.3366])),\n",
       "             ('fc3.weight', tensor([[ 0.3055,  0.0140, -0.2403],\n",
       "                      [ 0.3427,  0.4102, -0.0539],\n",
       "                      [-0.0157, -0.4691, -0.4021],\n",
       "                      [-0.0481, -0.5255,  0.1780],\n",
       "                      [-0.1348,  0.2751,  0.1359]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.1009, -0.4235,  0.1802, -0.4351,  0.4830]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#state_dict方法\n",
    "net1 = Q_Network(5, 5, [3, 3])\n",
    "net2 = Q_Network(5, 5, [3, 3])\n",
    "net1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0743, -0.0388,  0.2691,  0.2066,  0.3201],\n",
       "                      [ 0.0948, -0.3126,  0.0798, -0.0250, -0.1170],\n",
       "                      [-0.1719,  0.2674, -0.0075, -0.1921, -0.4443]])),\n",
       "             ('fc1.bias', tensor([ 0.0571,  0.2370, -0.2493])),\n",
       "             ('fc2.weight', tensor([[-0.4953,  0.2223, -0.0297],\n",
       "                      [-0.5063,  0.0753,  0.3710],\n",
       "                      [ 0.1346,  0.2840,  0.4755]])),\n",
       "             ('fc2.bias', tensor([ 0.1267,  0.3081, -0.1986])),\n",
       "             ('fc3.weight', tensor([[-0.0182, -0.4756,  0.0271],\n",
       "                      [-0.3871,  0.0410,  0.4056],\n",
       "                      [ 0.1736, -0.4287,  0.0648],\n",
       "                      [ 0.4501,  0.1102,  0.4227],\n",
       "                      [-0.0091,  0.2039,  0.5011]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.2565, -0.2944, -0.3965,  0.3402,  0.2100]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0925,  0.2964,  0.1767, -0.3285, -0.1470],\n",
       "                      [ 0.3120, -0.4361,  0.1911,  0.3987,  0.4299],\n",
       "                      [ 0.3245,  0.3840, -0.2999,  0.3200,  0.0381]])),\n",
       "             ('fc1.bias', tensor([ 0.2561, -0.3150,  0.3420])),\n",
       "             ('fc2.weight', tensor([[-0.5426, -0.5191,  0.4505],\n",
       "                      [-0.2780, -0.4256,  0.3189],\n",
       "                      [ 0.3753,  0.4871,  0.4037]])),\n",
       "             ('fc2.bias', tensor([ 0.2724, -0.1105, -0.3366])),\n",
       "             ('fc3.weight', tensor([[ 0.3055,  0.0140, -0.2403],\n",
       "                      [ 0.3427,  0.4102, -0.0539],\n",
       "                      [-0.0157, -0.4691, -0.4021],\n",
       "                      [-0.0481, -0.5255,  0.1780],\n",
       "                      [-0.1348,  0.2751,  0.1359]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.1009, -0.4235,  0.1802, -0.4351,  0.4830]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(net1.state_dict(), 'sample_weight.pth')\n",
    "net2.load_state_dict(torch.load('sample_weight.pth'))\n",
    "net2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .data and .data.copy_\n",
    "def soft_update(net1, net2, tau):\n",
    "    '''\n",
    "    This function update net2's weights as weighted average of net1 and net2's weights\n",
    "    net2 new weight = tau * net1 weight + (1 - tau) * net2 weight\n",
    "    '''\n",
    "    for target_param, local_param in zip(net2.parameters(), net1.parameters()):\n",
    "        target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.2299,  0.1428, -0.1564,  0.1906, -0.3330],\n",
       "                      [ 0.3900,  0.1510,  0.0532,  0.4311,  0.3918],\n",
       "                      [ 0.2452,  0.3879, -0.4217, -0.3185,  0.1902]])),\n",
       "             ('fc1.bias', tensor([0.3736, 0.1483, 0.3985])),\n",
       "             ('fc2.weight', tensor([[ 0.5117,  0.0028,  0.0388],\n",
       "                      [ 0.5616,  0.5186, -0.0247],\n",
       "                      [-0.3029,  0.2964,  0.4281]])),\n",
       "             ('fc2.bias', tensor([ 0.2233, -0.0437,  0.0111])),\n",
       "             ('fc3.weight', tensor([[ 0.4599, -0.3969,  0.5516],\n",
       "                      [-0.2753,  0.3211,  0.1092],\n",
       "                      [ 0.5005, -0.1821, -0.4430],\n",
       "                      [ 0.1723,  0.2741, -0.5264],\n",
       "                      [ 0.2439, -0.5012, -0.5745]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.1350,  0.2478,  0.3126,  0.2716, -0.3751]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Q_Network(5, 5, [3, 3])\n",
    "net2 = Q_Network(5, 5, [3, 3])\n",
    "net1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0014, -0.1165,  0.4389,  0.2044, -0.0413],\n",
       "                      [ 0.0307, -0.0345, -0.0496,  0.4184,  0.0350],\n",
       "                      [ 0.3984, -0.2312, -0.3261,  0.4412,  0.1084]])),\n",
       "             ('fc1.bias', tensor([ 0.1550, -0.0480,  0.3098])),\n",
       "             ('fc2.weight', tensor([[ 0.2695, -0.5170, -0.5436],\n",
       "                      [-0.1767, -0.1369, -0.0465],\n",
       "                      [ 0.4651, -0.3268,  0.5606]])),\n",
       "             ('fc2.bias', tensor([ 0.2329, -0.1796, -0.1200])),\n",
       "             ('fc3.weight', tensor([[-0.0162, -0.1983, -0.2579],\n",
       "                      [-0.1972,  0.2593, -0.3214],\n",
       "                      [ 0.0624, -0.2514,  0.0549],\n",
       "                      [-0.4351,  0.4384, -0.0198],\n",
       "                      [-0.3582, -0.0810, -0.1467]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.4678,  0.0820,  0.5584,  0.1007,  0.0040]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.1157,  0.0132,  0.1412,  0.1975, -0.1871],\n",
       "                      [ 0.2103,  0.0582,  0.0018,  0.4248,  0.2134],\n",
       "                      [ 0.3218,  0.0784, -0.3739,  0.0614,  0.1493]])),\n",
       "             ('fc1.bias', tensor([0.2643, 0.0501, 0.3542])),\n",
       "             ('fc2.weight', tensor([[ 0.3906, -0.2571, -0.2524],\n",
       "                      [ 0.1924,  0.1909, -0.0356],\n",
       "                      [ 0.0811, -0.0152,  0.4943]])),\n",
       "             ('fc2.bias', tensor([ 0.2281, -0.1116, -0.0544])),\n",
       "             ('fc3.weight', tensor([[ 0.2218, -0.2976,  0.1468],\n",
       "                      [-0.2362,  0.2902, -0.1061],\n",
       "                      [ 0.2814, -0.2167, -0.1941],\n",
       "                      [-0.1314,  0.3562, -0.2731],\n",
       "                      [-0.0572, -0.2911, -0.3606]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.3014,  0.1649,  0.4355,  0.1861, -0.1856]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_update(net1, net2, 0.5)\n",
    "net2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.2299,  0.1428, -0.1564,  0.1906, -0.3330],\n",
       "                      [ 0.3900,  0.1510,  0.0532,  0.4311,  0.3918],\n",
       "                      [ 0.2452,  0.3879, -0.4217, -0.3185,  0.1902]])),\n",
       "             ('fc1.bias', tensor([0.3736, 0.1483, 0.3985])),\n",
       "             ('fc2.weight', tensor([[ 0.5117,  0.0028,  0.0388],\n",
       "                      [ 0.5616,  0.5186, -0.0247],\n",
       "                      [-0.3029,  0.2964,  0.4281]])),\n",
       "             ('fc2.bias', tensor([ 0.2233, -0.0437,  0.0111])),\n",
       "             ('fc3.weight', tensor([[ 0.4599, -0.3969,  0.5516],\n",
       "                      [-0.2753,  0.3211,  0.1092],\n",
       "                      [ 0.5005, -0.1821, -0.4430],\n",
       "                      [ 0.1723,  0.2741, -0.5264],\n",
       "                      [ 0.2439, -0.5012, -0.5745]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.1350,  0.2478,  0.3126,  0.2716, -0.3751]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_update(net1, net2, 1)\n",
    "net2.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Buffer**\n",
    "\n",
    "`collections`的`deque`只有一个主要参数，就是`maxlen`，也就是最大储存数量\n",
    "\n",
    "`deque`和`list`几乎完全一致，只是会自动控制最大元素个数，所以很好使用，也很适合作为Replay Buffer使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=3)\n",
      "deque([0, 1], maxlen=3)\n",
      "deque([0, 1, 2], maxlen=3)\n",
      "deque([1, 2, 3], maxlen=3)\n",
      "deque([2, 3, 4], maxlen=3)\n",
      "deque([3, 4, 5], maxlen=3)\n",
      "deque([4, 5, 6], maxlen=3)\n",
      "deque([5, 6, 7], maxlen=3)\n",
      "deque([6, 7, 8], maxlen=3)\n",
      "deque([7, 8, 9], maxlen=3)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "test = deque(maxlen=3)\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) loss计算\n",
    "\n",
    "### 1) `torch.gather`\n",
    "\n",
    "此处要强调loss计算是因为，我们的网络对于输入$s_t$，会直接输出所有的$$Q(s_t, a_1), Q(s_t, a_2), ..., Q(s_t, a_n)$$\n",
    "为了将一个batch中需要的Q-values便捷的提取出来，需要介绍一个一个函数`torch.gather`\n",
    "\n",
    "`torch.gather`的一般用法为`torch.gather(input, dim, index)`，这个函数主要的功能是根据`dim`和`index`来提取`input`中的值，具体的来说：\n",
    "  * `input`和`index`必须拥有同样的size，$(d_0, d_1, ..., d_k)$，除了`dim`维度\n",
    "  * `dim`的范围不能超过size的长度，即上一行的$k$\n",
    "  * `input`:一个`torch.tensor`对象，取值的来源\n",
    "  * `dim`:一个整数，被取代的维度，之后补充具体意义\n",
    "  * `index`:一个`torch.tensor`对象，必须是`torch.long`数据类型，用来指定提取的坐标\n",
    "  * 数学表达式来看，函数返回值仍是一个$(d_0, d_1, ..., d_k)$维度，和`input`有相同数据类型的`torch.tensor`，不过在$[i_0, i_1,...,i_k]$坐标上的值是`input`对应坐标$[i_0, i_1, ...,i_{dim-1}, index[i_0, i_1,...,i_k], i_{dim+1},...,i_k]$\n",
    "  * 简单来说，就是用`index`的值去改变原坐标中`dim`维度的值，这也是`index`需要是`torch.long`类型的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7587, 0.2253, 0.1851, 0.1033],\n",
       "        [0.5584, 0.7686, 0.8005, 0.7403],\n",
       "        [0.9221, 0.7041, 0.4943, 0.2212],\n",
       "        [0.7942, 0.6475, 0.0248, 0.5439],\n",
       "        [0.9903, 0.1416, 0.8074, 0.6079]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "test = torch.rand((5, 4))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([[1,2,2,0],\n",
    "                      [3,1,1,2],\n",
    "                      [0,0,0,0],\n",
    "                      [1,1,1,1],\n",
    "                      [2,2,2,2]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5584, 0.7041, 0.4943, 0.1033],\n",
       "        [0.7942, 0.7686, 0.8005, 0.2212],\n",
       "        [0.7587, 0.2253, 0.1851, 0.1033],\n",
       "        [0.5584, 0.7686, 0.8005, 0.7403],\n",
       "        [0.9221, 0.7041, 0.4943, 0.2212]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(test, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2253, 0.1851, 0.1851, 0.7587],\n",
       "        [0.7403, 0.7686, 0.7686, 0.8005],\n",
       "        [0.9221, 0.9221, 0.9221, 0.9221],\n",
       "        [0.6475, 0.6475, 0.6475, 0.6475],\n",
       "        [0.8074, 0.8074, 0.8074, 0.8074]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(test, 1, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) `torch.max`\n",
    "\n",
    "`torch.max`比较简单，主要的使用方法是`torch.max(input, dim)`，作用就是沿着`dim`维度提取每个slice的最大值即对应下标，由于函数本身比较简单，就不单独给出样例了\n",
    "\n",
    "下面我们进行两个具体的操作：\n",
    "  * 提取`test`每一行的最大值\n",
    "  * 提取`test`对应行的第(2,2,1,3,0)元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7587, 0.8005, 0.9221, 0.7942, 0.9903]), tensor([0, 2, 0, 0, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max\n",
    "values, indices = torch.max(test, dim=1)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1851],\n",
       "        [0.8005],\n",
       "        [0.7041],\n",
       "        [0.5439],\n",
       "        [0.9903]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[2], [2], [1], [3], [0]], dtype=torch.long)\n",
    "torch.gather(test, 1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 4: Index tensor must have same dimensions as input tensor at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:638",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aebd903616c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 4: Index tensor must have same dimensions as input tensor at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:638"
     ]
    }
   ],
   "source": [
    "index = torch.tensor([2, 2, 1, 3, 0], dtype=torch.long)\n",
    "torch.gather(test, 1, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由这两个函数，我们就可以计算loss了，具体的过程为：\n",
    "  * 已知mini-batch中的state, action, next_state, reward, done\n",
    "  * $Q$网络对应`net_local`，$\\hat{Q}$网络对应`net_target`\n",
    "  * 计算Q-values `Q_values = net_local(state)`\n",
    "  * 根据action，提取对应的Q-values `Q_values = torch.gather(Q_values, 1, action)`\n",
    "  * 计算next_state对应的最大的target values `Q_targets, _ = torch.max(net_target(next_state), 1, keepdim=True)`\n",
    "  * 在target values中加入reward和done的信息 `Q_targets = reward + (1 - done) * Q_targets`\n",
    "  * 注意`Q_targets`不应该有导数\n",
    "  * 计算loss `loss = (Q_values - Q_targets).pow(2).mean()`\n",
    "  * 使用`.backward()`方法更新参数\n",
    "  \n",
    "## 4) 训练过程\n",
    "\n",
    "### 1) 结构分析\n",
    "之前的notebook中我们已经示范了如何获得随机策略的reward平均值，我们将在这个的基础上扩充代码，将其变为可以用做训练的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        #env.render()\n",
    "        action = env.action_space.sample() # random action\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        episodic_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的代码中，我们已经完成的部分有：\n",
    "  * 初始化参数\n",
    "  * 初始化environment\n",
    "  * 选择action，记录reward\n",
    " \n",
    "现在我们需要增加的部分有：\n",
    "  * 初始化和调整$\\epsilon$的值以实现$\\epsilon$-Greedy\n",
    "  * agent选择action部分\n",
    "  * agent的训练部分，即更新$Q$和$\\hat{Q}$的对应参数\n",
    "  * 监督训练进度的部分\n",
    "\n",
    "之前我们已经分析了`agent`的结构，如果我们认为我们已经完成了`agent`类的设计，那么具体的任务就是：\n",
    "  * 初始化和调整$\\epsilon$的值以实现$\\epsilon$-Greedy\n",
    "  * 在合适的位置插入`agent.act()`\n",
    "  * 在合适的位置插入`agent.learn()`\n",
    "  * 监督训练进度的部分\n",
    "  \n",
    "## 2) 具体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize \n",
    "    state = env.reset()\n",
    "    # preprocessing of state if necessary\n",
    "    t = 0\n",
    "    done = False\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        action = agent.act(state, eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # preprocessing of next_state if necessary\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        if t % C == 0:\n",
    "            agent.learn()\n",
    "            agent.soft_update()\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    reward_log.append(episodic_reward)\n",
    "    \n",
    "    # monitor\n",
    "    average_log.append(np.mean(rewards_log[-100:])) \n",
    "    print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "    if i % 50 == 0:\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
