# 第六章 时序差分学习 习题解答



## 练习6.1

> 如果 $V$ 在幕中发生变化，那么式 (6.6) 只能近似成立；等式两侧差在哪里？令 $V_t$ 表示在 $t$ 时刻在 TD 误差公式  (6.5) 和 TD 更新公式 (6.2) 中使用的状态价值的数组。重新进行上面的推导，推出为了让等式右侧仍等于左侧的蒙特卡洛误差，需要在 TD 误差之和上加上的额外项。

*解答：* 由式 (6.2) 得
$$
V_{t+1}(S_{t+1})=V_t(S_{t+1})+\alpha\big(R_{t+2}+\gamma V_t(S_{t+2})-V_t(S_{t+1})\big)
$$
那么
$$
\begin{align}
G_t-V_t(S_t)&=R_{t+1}+\gamma G_{t+1}-V(S_t)+\gamma V_t(S_{t+1})-\gamma V_t(S_{t+1})\\
&=\delta_t+\gamma\big(G_{t+1}-V_t(S_{t+1})\big)\\
&=\delta_t+\gamma\Big(G_{t+1}-V_{t+1}+\alpha \big(R_{t+2}+\gamma V_t(S_{t+2})-V_t(S_{t+1}) \big) \Big)\\
&=\delta_t+\gamma(G_{t+1}-V_{t+1})+\gamma \alpha \big(R_{t+2}+\gamma V_t(S_{t+2})-V_t(S_{t+1}) \big)\\
&\dots\\
&=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k+\sum_{k=t}^{T-2}\gamma^{k-t+1}\alpha \big(R_{k+2}+\gamma V_k(S_{k+2})-V_k(S_{k+1}) \big)
\end{align}
$$

## 练习6.2

> 这个练习是为了帮助你更好地从直觉上理解为什么 TD 方法通常比蒙特卡洛方法更高效。首先仔细回顾和思考一下 TD 方法和蒙特卡洛方法是如何应用在开车回家这个例子中的。你能想到一个使用 TD 更新会在平均意义上比使用蒙特卡洛更新更好的场景吗？请举一个你觉得 TD 更新会更好的例子，例子中需要包括对过去经验的描述以及当前的状态。提示：假设你有很多次从办公室驾车回家的经历，某天你搬到一个新的办公楼和一个新的停车场（但仍从同一个入口上高速）。现在你开始学习从新办公楼回家的时间预测。在这种情况下，你能否知道为什么 TD 更新至少在一开始时可能会好得多？同样的事情是不是可能在原来的任务中发生过呢？

*解答：*正如提示中所述，当我们搬到新办公楼与新停车场后，考虑从办公室到高速入口这段路程，如果用蒙特卡洛方法，这段路程中各状态的价值函数需要根据对全程的重新采样来学习；而在 TD 方法中，搬离前后从高速入口到家这段路程的状态与价值是不变的，因此 TD 方法除了根据新的采样学习外，还可以沿用从高速入口到家这段路程中的价值进行学习，实现更好的效果。

## 练习6.3

> 在随机游走例子的左图中，第一幕只导致V(A) 发生了变化。由此，你觉得第一幕发生了什么？为什么只有这一个状态的估计值改变了？它的值到底改变了多少？

*解答：*将两侧端点分别记为 l,r。在本例中 $\gamma=1$，因此更新公式为
$$
V(S)\leftarrow V(S)+\alpha[R+V(S')-V(S)]
$$
不难发现，当沿 $A\to B$， $B\to C$， $C\to D$， $D\to E$，$E\to D$， $D\to C$， $C\to B$， $B\to A$ 进行转移时，$R+V(S')-V(S)=0$，因此不会改变状态价值；

当沿 $A\to l$ 进行转移时，
$$
V(A)\leftarrow V(A)+\alpha[R+V(l)-V(A)]=0.5+0.1\times(0+0-0.5)=0.45
$$
类似地，当沿 $E\to r$ 进行转移时，
$$
V(E)\leftarrow V(E)+\alpha[R+V(r)-V(E)]=0.5+0.1\times(1+0-0.5)=0.55
$$
从图中可以看出，$V(A)$ 发生变化，$V(E)$ 未发生变化，说明在第一幕中，幕终止于最左侧。

## 练习6.4

> 在随机游走例子的右图中，具体结果与步长参数 $\alpha$ 的值是相关的。如果 $\alpha$ 从一个更宽广的值域中取值，会影响到哪种算法更好的结论吗？是否存在另一个固定的 $\alpha$，使得两种算法都能表现出比图中更好的性能？为什么？

*解答：*即使 $\alpha$ 从一个更宽广的值域中取值，TD 的效果明显优于 MC。较大的 $\alpha$ 能够使模型更快地收敛，但误差较大；较小的 $\alpha$ 收敛较慢，但误差较小。因此 $\alpha$ 的值是根据具体问题与需求进行调节的超参数，并不存在固定的最优 $\alpha$.

## 练习6.5

> 在随机游走例子的右图中，TD 方法的均方根误差先下降后上升，尤其在 $\alpha$ 较大时更明显。这是由什么导致的？你认为这种情况总是会发生，还是与近似价值函数如何初始化有关？

*解答：*观察 TD 方法的更新公式
$$
V(S)\leftarrow V(S)+\alpha[R+\gamma V(S')-V(S)]
$$
将第 $i$ 次更新中 $R+\gamma V(S')$ 的值记为 $u_i$，将 $V(S)$ 的初始化值记为 $u_0$，那么在 $n$ 次更新 $V(S)$ 后
$$
V(S)=(1-\alpha)^nu_0+\alpha(1-\alpha)^{n-1}u_1+\alpha(1-\alpha)^{n-2}u_2+\dots+\alpha u_n
$$
$V(S)$ 是 $u_0,u_1,\dots,u_n$ 的加权平均，其权重为 $(1-\alpha)^n$，$\alpha(1-\alpha)^{n-1}$，$\alpha(1-\alpha)^{n-2}$，$\dots$，$\alpha$。当使用较大的 $\alpha$ 值时，位置靠后的 $u_i$ 的权重较大，因此 $V(S)$ 的值随迭代发生震荡；当使用较小的 $\alpha$ 值时，权重更加“均匀”，$V(S)$ 的值更稳定，震荡幅度较小。因此 TD 方法的误差先下降，后在震荡中略微上升。

另外，观察到初始化值 $u_0$ 的权重为 $(1-\alpha)^n$，$\alpha$ 越小，$u_0$ 的权重越大，收敛越慢，但 $\lim_{n\to \infty}(1-\alpha)^n=0$，$u_0$ 的影响会越来越小。因此无论 $V(S)$ 如何初始化，都无法避免误差先下降后上升。

## 练习6.6

> 在例 6.2 中的随机游走任务中，状态 A~E 的真实价值分别是 $\frac{1}{6},\frac{2}{6},\frac{3}{6},\frac{4}{6}$ 和 $\frac{5}{6}$。描述至少两种不同的计算这些值的方法。猜猜实际中我们使用的是哪种方法？为什么？

*解答：*一种方法是利用期望的线性性，由  $ v_{\pi}(s) $    的定义知
$$
\begin{align}
v_{\pi}(s)&=\mathbb E_{\pi}(G_t|S_t=s)\\
&=\sum_{s'} \mathbb E_{\pi}(G_{t+1}|S_{t+1}-s')P(S_{t+1}=s'|S_t=s)\\
&=\sum_{s'}v_{\pi}(s')P(S_{t+1}=s'|S_t=s)
\end{align}
$$
在本题中有
$$
\begin{cases}
v_{\pi}(A)=\frac{1}{2}\cdot 0+\frac{1}{2}v_{\pi}(B)\\
v_{\pi}(B)=\frac{1}{2}v_{\pi}(A)+\frac{1}{2}v_{\pi}(C)\\
v_{\pi}(C)=\frac{1}{2}v_{\pi}(B)+\frac{1}{2}v_{\pi}(D)\\
v_{\pi}(D)=\frac{1}{2}v_{\pi}(C)+\frac{1}{2}v_{\pi}(E)\\
v_{\pi}(E)=\frac{1}{2}v_{\pi}(D)+\frac{1}{2}\cdot 1
\end{cases}
$$
解得 $v_{\pi}(A)=\frac{1}{6}$，$v_{\pi}(B)=\frac{2}{6}$，$v_{\pi}(C)=\frac{3}{6}$，$v_{\pi}(D)=\frac{4}{6}$，$v_{\pi}(E)=\frac{5}{6}$.

另一种方法略微复杂，利用离散马尔科夫链的相关理论，转移矩阵为
$$
P=\begin{pmatrix}
1&0&0&0&0&0&0\\
\frac{1}{2}&0&\frac{1}{2}&0&0&0&0\\
0&\frac{1}{2}&0&\frac{1}{2}&0&0&0\\
0&0&\frac{1}{2}&0&\frac{1}{2}&0&0\\
0&0&0&\frac{1}{2}&0&\frac{1}{2}&0\\
0&0&0&0&\frac{1}{2}&0&\frac{1}{2}\\
0&0&0&0&0&0&1
\end{pmatrix}
$$
其中位置与下标的对应关系为

| 左侧端点 | $A$  | $B$  | $C$  | $D$  | $E$  | 右侧端点 |
| -------- | ---- | ---- | ---- | ---- | ---- | -------- |
| 1        | 2    | 3    | 4    | 5    | 6    | 7        |

对 $P$ 进行对角化得
$$
P=Q\Sigma Q^{-1}
$$
其中
$$
Q=\left(
\begin{array}{ccccccc}
 0 & 0 & 0 & -5 & 6 & 0 & 0 \\
 -1 & 1 & -1 & -4 & 5 & 1 & 1 \\
 1 & 0 & -1 & -3 & 4 & -\sqrt{3} & \sqrt{3} \\
 0 & -1 & 0 & -2 & 3 & 2 & 2 \\
 -1 & 0 & 1 & -1 & 2 & -\sqrt{3} & \sqrt{3} \\
 1 & 1 & 1 & 0 & 1 & 1 & 1 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
\end{array}
\right)
$$

$$
\Sigma=\left(
\begin{array}{ccccccc}
 -\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & \frac{1}{2} & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & -\frac{\sqrt{3}}{2} & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & \frac{\sqrt{3}}{2} \\
\end{array}
\right)
$$
那么
$$
P^n=Q\Sigma^n Q^{-1}
$$

$$
\lim_{n\to \infty}P^n=\left(
\begin{array}{ccccccc}
 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 \frac{5}{6} & 0 & 0 & 0 & 0 & 0 & \frac{1}{6} \\
 \frac{2}{3} & 0 & 0 & 0 & 0 & 0 & \frac{1}{3} \\
 \frac{1}{2} & 0 & 0 & 0 & 0 & 0 & \frac{1}{2} \\
 \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & \frac{2}{3} \\
 \frac{1}{6} & 0 & 0 & 0 & 0 & 0 & \frac{5}{6} \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{array}
\right)
$$
以 $B$ 点为例，计算以 $B$ 为起点，经过 $n+1$ 步后停留在各处的概率，即为
$$
(0,0,1,0,0,0,0)P^n
$$
经过足够的步数后，即 $n\to \infty$，有
$$
\lim_{n\to \infty}(0,0,1,0,0,0,0)P^n=\left(\frac{2}{3},0,0,0,0,0,\frac{1}{3}\right)
$$
因此从 $B$ 点出发，停留在左侧的概率为 $\frac{2}{3}$，停留在右侧的概率为 $\frac{1}{3}$，$v_{\pi}(B)=\frac{2}{3}\cdot 0+\frac{1}{3}\cdot 1=\frac{1}{3}$，类似地可以得到其他点的真实价值。

## *练习6.7

> 设计一个离轨策略版的 TD(0) 更新算法，使其可以用于任意的目标策略 $\pi$，且公式包含行动策略 $b$。注意：在每个时刻 $t$ 要使用重要度采样比 $\rho_{t:t}$（式 5.3）

*解答：*

输入：待评估的目标策略 $\pi$，行动策略 $b$
算法参数：步长 $\alpha\in(0,1]$
对于所有 $s\in S^+$，任意初始化 $V(S)$，其中 $V(终止状态)=0$
对每幕循环：
​	初始化 $S$
​	对幕中的每一步循环：
​		$A\leftarrow$ 策略 $b$ 在状态 $S$ 下做出的决策动作
​		执行动作 $A$，观察到 $R$，$S'$
​		$\rho_{t:t}\leftarrow \frac{\pi(A|S)}{b(A|S)}$
​		$V(S)\leftarrow (1-\alpha)V(S)+\alpha \rho_{t:t}[R+\gamma V(S')]$
​		$S\leftarrow S'$
​	直到 $S$ 为终止状态

## 练习6.8

> 证明公式 (6.6) 的“状态-动作”二元组版本也是成立的，“状态-动作”二元组的 TD 误差是 $\delta_t=R_{t+1}+\gamma Q(S_{t+1}A_{t+1})-Q(S_t A_t)$。注意，这里我们同样假设价值函数在不同时刻不发生改变。

*解答：*
$$
\begin{align}
G_t-Q(S_t,A_t)&=R_{t+1}+\gamma G_{t+1}-Q(S_t,A_t)+\gamma Q(S_{t+1},A_{t+1})-\gamma Q(S_{t+1},A_{t+1})\\
&=\delta_t+\gamma\big(G_{t+1}-Q(S_{t+1},A_{t+1}) \big)\\
&=\delta_t+\gamma\delta_{t+1}+\gamma^2\big(G_{t+2}-Q(S_{t+2},A_{t+2}) \big)\\
&=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}\big(G_T-Q(S_T,A_T)\big)\\
&=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}\big(0-0\big)\\
&=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k
\end{align}
$$

## 练习6.9

> *在有风的网格世界中向 $8$ 个方向移动（编程）* 重新解决有风的网格世界问题。这次假设可以向包括对角线在内的 $8$ 个方向移动，而不是原来的 $4$ 个方向。利用更多的动作，你得到的结果比原来能好多少？如果还有第 $9$ 个动作，即除了风造成的移动之外不做任何移动，你能进一步获得更好的结果吗？

[练习6.9代码](代码案例.md)

## 练习6.10

> *随机强度的风（编程）* 重新解决在有风的网格世界中向 $8$ 个方向移动的问题。假设有风且风的强度是随机的，即在每列的平均值上下可以有 $1$ 的变化。也就是说，有三分之一的时间你在和之前问题中一样，精确地按照每列下标给出的数值移动，但另有三分之一的时间你会朝上多移动一格，还有三分之一的时间你会朝下多移动一格。举个例子，如果你在目标状态右边一格，你向左移动，那么在三分之一的时间你会移动到目标上方的一格，在另一个三分之一的时间你会移动到目标上方的两格，在剩下的三分之一的时间你会正好移动到目标。

[练习6.10代码](代码案例.md)

## 练习 6.11

> 为什么 Q 学习被认为是一种离轨策略的控制方法？

*解答：*在 Q 学习的更新公式中，增量部分为 $R+\gamma\max_aQ(S',a)$，其中 $\max_aQ(S',a)$ 是完全根据贪心策略得到的，但在生成采样数据序列时，使用的是某一“从 Q 得到的策略”（例如 $\epsilon$-贪心）而非贪心策略，因此用于生成采样数据序列的策略与用于实际决策的策略是不同的，Q 学习是离轨策略方法。

## 练习6.12

> 假设使用贪心的方式选择动作，那么此时 Q 学习和 Sarsa 是完全相同的算法吗？它们会做出完全相同的动作选择，进行完全相同的权重更新吗？

*解答：*当使用贪心的方式选择动作时，Q 学习与 Sarsa 并不是完全相同的算法。考虑一种特殊情况，在状态 $S$ 下，使用贪心方式选择了动作 $A$，转移到状态 $S'=S$，此时 $Q(S,\cdot)$ 已经被更新，在 Sarsa 算法中，下一步采取的策略是根据更新前的 $Q(S,\cdot)$ 得到的，而在 Q 学习中，下一步采取的策略是根据更新后的 $Q(S,\cdot)$ 得到的。另外，即使是相同的算法，由于算法本身具有随机性，因此并不会做出完全相同的动作选择，当然也不会进行完全相同的权重更新。

## *练习6.13

> 使用 $\epsilon$-贪心目标策略的双期望 Sarsa 的更新步骤是怎样的？

*解答：*

算法参数：步长 $\alpha\in (0,1]$，很小的 $\epsilon$，$\epsilon>0$
对所有 $s\in S^+$，$a\in \mathcal A(s)$，初始化 $Q_1(s,a)$ 和 $Q_2(s,a)$，其中 $Q(终止状态,\cdot)=0$
对每幕循环：
​	初始化 $S$
​	对幕中每一步循环：
​		$\pi(a|S)=\begin{cases}1-\epsilon+\frac{\epsilon}{\mathcal A(S)},&\text{if }a=\arg\max_b\big(Q_1(S,b)+Q_2(S,b)\big)\\ \frac{\epsilon}{\mathcal A(S)},&\text{otherwise} \end{cases}$
​		基于 $Q_1+Q_2$，使用策略 $\pi$ 在 $S$ 中选择 $A$
​		执行动作 $A$，观察到 $R,S'$
​		以 $0.5$ 的概率执行：
​			$Q_1(S,A)\leftarrow Q_1(S,A)+\alpha\big(R+\gamma \sum_a \pi(a|S')Q_2(S',a)-Q_1(S,A) \big)$
​		或者执行：
​			$Q_2(S,A)\leftarrow Q_2(S,A)+\alpha\big(R+\gamma \sum_a \pi(a|S')Q_1(S',a)-Q_2(S,A) \big)$
​		$S\leftarrow S'$
​	直到 $S$ 是终止状态