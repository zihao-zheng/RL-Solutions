# 第二章 多臂赌博机

本章通过一个简单的例子：多臂赌博机，来介绍强化学习和其他类型的学习的主要区别。

假设玩家有$k$种动作，在$t$时间内可以在$k$种动作中选择其一，每一种动作对应一种奖励。这就是$k$-臂赌博机问题。

对于任意的动作$a$，其在时刻$t$的奖励的期望值为：
$$q_*(a)=\mathbb{E}[R_t|A_t=a]$$

如果已知每一个动作的奖励值，则问题很简单：总是选择奖励最高的动作即可。现在假设我们不知道真实的$q_*(a)$，我们用$Q_t(a)$来表示动作$a$在时刻$t$的奖励估计值，其可以通过实际获得的奖励来自然地计算出来：
$$Q_t(a)=\frac{\text{sum of rewards when }a\text{ taken prior to }t}{\text{number of times }a\text{ taken prior to }t}=\frac{\sum_{i=1}^{t-1}R_i\cdot\mathbbm{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbbm{1}_{A_i=a}}$$

在已知所有时刻和动作下的奖励估计值后，根据**贪心**策略，我们可以一直选择估计奖励最高的动作。

跟贪心策略相反，我们通过随机选择动作可以**探索**出期望奖励最高的动作。

贪心策略可以在短期内获得较高奖励，而探索策略在长期来看更为有利。因此我们可以根据动作数和拥有的时间来平衡前期的探索策略和后期的贪心策略，以达到最高的奖励值。


## 本节目录

1. [习题解答](习题解答.md)

2. [代码案例](代码案例.md)
